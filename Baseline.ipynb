{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ff06d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import re, string\n",
    "import sys\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82280ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def init_dataset(json) -> tuple[dict, list]:\n",
    "    ds: dict = {}\n",
    "    keys = json.keys()\n",
    "    for k in keys:\n",
    "        ds[k] = []\n",
    "    return ds, keys\n",
    "\n",
    "def read_json(file) -> pd.DataFrame:\n",
    "    dataset = {}\n",
    "    keys = []\n",
    "    with open(file) as file_lines:\n",
    "        for count, line in enumerate(file_lines):\n",
    "            json_line = json.loads(line.strip())\n",
    "            if count == 0:\n",
    "                dataset, keys = init_dataset(json_line)\n",
    "            for k in keys:\n",
    "                dataset[k].append(json_line[k])\n",
    "        return pd.DataFrame(dataset)\n",
    "\n",
    "def read_csv(file) -> pd.DataFrame:\n",
    "    dataset = {}\n",
    "    with open(file, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        keys = reader.fieldnames\n",
    "        for k in keys:\n",
    "            dataset[k] = []\n",
    "        for row in reader:\n",
    "            for k in keys:\n",
    "                dataset[k].append(row[k])\n",
    "    return pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e009e05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Read In Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc0257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp_review = read_json('data/yelp_academic_dataset_review.json')\n",
    "yelp_review = read_csv('data/yelp_academic_dataset_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e0a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp_business = read_json('data/yelp_academic_dataset_business.json')\n",
    "yelp_business = read_csv('data/yelp_academic_dataset_business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd32bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data: Restaurants reviewed by karen, the user with the most reviews\n",
    "# Businesses that are categorized as restaurants\n",
    "business_restaurant = yelp_business.loc[yelp_business['categories'].str.contains('Restaurant', na=False)]\n",
    "# Reviews of Restaurant businesses\n",
    "review_restaurant = yelp_review[yelp_review['business_id'].isin(business_restaurant['business_id'])]\n",
    "# User with most restaurant reviews\n",
    "karen = review_restaurant['user_id'].value_counts().index[0]\n",
    "# Reviews Karen has made of restaurant businesses\n",
    "review_restaurant_karen = review_restaurant.loc[review_restaurant['user_id'] == karen]\n",
    "# Restaurant businesses that Karen has reviewed\n",
    "business_restaurant_karen = business_restaurant[business_restaurant['business_id'].isin(review_restaurant_karen['business_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499184c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b52787",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Data: remove missing rows and irrelevant columns\n",
    "df = business_restaurant_karen.set_index('business_id')\n",
    "\n",
    "# Remove columns with greater than 20% missing fields\n",
    "mask = df.applymap(lambda x: x =='' or x == 'None').sum()\n",
    "features = ((mask/len(df)) * 100).map(lambda x: x < 20)\n",
    "\n",
    "\n",
    "# Remove non-attribute columns (except business_id)\n",
    "features.loc[~features.index.str.contains('attributes.')] = False\n",
    "#features.loc['business_id'] = True\n",
    "dataset = df.loc[:, features]\n",
    "\n",
    "# Remove rows with missing data\n",
    "mask = dataset.applymap(lambda x: x == '' or x == 'None')\n",
    "dataset = dataset.loc[~mask.any(axis=1)]\n",
    "\n",
    "# Remove all non-boolean columns\n",
    "mask = dataset.applymap(lambda x : x == 'True' or x == 'False').sum() != 0\n",
    "#mask.loc['business_id'] = True\n",
    "#dataset = dataset.set_index('business_id')\n",
    "dataset = dataset.loc[:, mask].applymap(lambda x: x == 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebaeb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Data: add targets\n",
    "df = review_restaurant_karen.set_index('business_id')\n",
    "df = df.loc[df.index.intersection(dataset.index)]\n",
    "df = df.astype({'stars':'float'})\n",
    "dataset['target'] = df.groupby(df.index)['stars'].mean().map(lambda x: x > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature set to numpy array\n",
    "lamb = lambda x: 1 if x == True else 0\n",
    "labels = dataset['target'].map(lamb).to_numpy()\n",
    "feature_set = dataset.drop(['target'], axis=1).applymap(lamb).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a95bde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform a Principal Component Analysis on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5685b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "# Calculate eigenvectors of covariance matrix\n",
    "C = np.cov(feature_set.T)\n",
    "evals, evecs = np.linalg.eig(C)\n",
    "pcts = 100 * evals / np.sum(evals)\n",
    "\n",
    "# Select Feature Matrix\n",
    "sortidx = np.argsort(pcts)\n",
    "sorted_evecs = evecs[sortidx[::-1]]\n",
    "feature_vec = sorted_evecs[:6]\n",
    "\n",
    "X_pca = feature_vec @ feature_set.T\n",
    "tmp_df = pd.DataFrame(X_pca.T)\n",
    "tmp_df['target'] = labels\n",
    "#pcts[sortidx[::-1]][:6], X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c04e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "#X_scaled = scaler.fit_transform(feature_set)\n",
    "X_scaled = feature_set\n",
    "\n",
    "# Select the top n principal components\n",
    "n_components = 6\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "tmp_df = pd.DataFrame(X_pca)\n",
    "tmp_df['target'] = labels\n",
    "#pca.explained_variance_ratio_ * 100, X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a680973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = dataset.applymap(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83316443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delineate between traning and testing set\n",
    "train_df, test_df = train_test_split(tmp_df, test_size=0.2, random_state=42)\n",
    "y_train = train_df['target'].to_numpy()\n",
    "x_train = train_df.drop(['target'], axis=1).to_numpy()\n",
    "\n",
    "y_test = test_df['target'].to_numpy()\n",
    "x_test = test_df.drop(['target'], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969036f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Logistic Regression Classifier using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4019937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis function... sigmoid function\n",
    "def g(theta, x):\n",
    "    return 1 / (1 + np.exp(-x @ theta))\n",
    "\n",
    "# matrix derivative\n",
    "def dJ(theta, x, y):\n",
    "    m, _ = x.shape\n",
    "    return 1/m* x.T @ (g(theta, x) - y)\n",
    "\n",
    "# hessian matrix\n",
    "def HJ(theta, x):\n",
    "    m, _ = x.shape\n",
    "    Z = g(theta, x)\n",
    "    Z = Z*(1-Z)\n",
    "    return 1/m * Z * x.T @ x\n",
    "\n",
    "# distance between two vectors\n",
    "def dist(x, y):\n",
    "    return np.sum(np.abs(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb794e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, step_size=0.2, max_iter=100, eps=1e-5,\n",
    "                theta_0=None, verbose=True):\n",
    "        self.theta = theta_0\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        m, n = x.shape\n",
    "        if self.theta is None:\n",
    "            self.theta=np.zeros(n)\n",
    "        for i in range(self.max_iter):\n",
    "            theta_new = self.theta - np.linalg.inv(HJ(self.theta, x)) @ dJ(self.theta, x, y)\n",
    "            if dist(theta_new, self.theta) < self.eps:\n",
    "                self.theta = theta_new\n",
    "                break\n",
    "            else:\n",
    "                self.theta = theta_new\n",
    "\n",
    "    def predict(self, x):\n",
    "        return x @ self.theta >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4ca8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta:  [ 0.59714209  0.49508892 -0.042109   -0.08595731  0.51446855 -0.48017102\n",
      "  0.13016552 -0.64661731 -0.19983955  0.52538337]\n",
      "Training accuracy:  0.6076642335766423\n",
      "Testing accuracy:   0.6014492753623188\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(x_train, y_train)\n",
    "print(\"Theta: \", lg.theta)\n",
    "print(\"Training accuracy: \", np.mean(lg.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(lg.predict(x_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a589d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f652b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Naive Bayes Classifier\n",
    "'''\n",
    "class NaiveBayes:\n",
    "    '''\n",
    "    Naive Bayes Classifier (Bernoulli event model)\n",
    "\n",
    "    During training, the classifier learns probabilities by counting the\n",
    "    occurences of feature/label combinations that it finds in the\n",
    "    training data. During prediction, it uses these counts to\n",
    "    compute probabilities.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, use_laplace_add_one):\n",
    "        self.label_counts = {}\n",
    "        self.feature_counts = {}\n",
    "        self.use_laplace_add_one = use_laplace_add_one # True for Laplace add-one smoothing\n",
    "\n",
    "    def fit(self, train_features, train_labels):\n",
    "        '''Training stage - learn from data'''\n",
    "\n",
    "        self.label_counts[0] = 0\n",
    "        self.label_counts[1] = 0\n",
    "\n",
    "        self.label_counts[0] = np.count_nonzero(train_labels == 0)\n",
    "        self.label_counts[1] = np.count_nonzero(train_labels == 1)\n",
    "\n",
    "        for row, sample in enumerate(train_features):\n",
    "            label = train_labels[row]\n",
    "            for feature, feature_value in enumerate(sample):\n",
    "                key = (feature, feature_value, label)\n",
    "                self.feature_counts[key] = self.feature_counts.get(key, 0) + 1\n",
    "\n",
    "    def predict(self, test_features):\n",
    "        '''Testing stage - classify new data'''\n",
    "\n",
    "        preds = np.zeros(test_features.shape[0], dtype=np.uint8)\n",
    "\n",
    "        tot = self.label_counts[0] + self.label_counts[1]\n",
    "        p_y0 = self.label_counts[0] / tot\n",
    "        p_y1 = self.label_counts[1] / tot\n",
    "        for row, sample in enumerate(test_features):\n",
    "            p_y0_mid_x = p_y0\n",
    "            p_y1_mid_x = p_y1\n",
    "            for feature, feature_value in enumerate(sample):\n",
    "                #calc prob sample 0\n",
    "                xi = (feature, feature_value, 0)\n",
    "                c_xi_and_y0 = self.feature_counts.get(xi, 0)\n",
    "\n",
    "                #calc prob sample 1 \n",
    "                xi = (feature, feature_value, 1)\n",
    "                c_xi_and_y1 = self.feature_counts.get(xi, 0)\n",
    "\n",
    "                if (self.use_laplace_add_one):\n",
    "                    p_y0_mid_x *= (c_xi_and_y0 + 1) / (self.label_counts[0] + 2)\n",
    "                    p_y1_mid_x *= (c_xi_and_y1 + 1) / (self.label_counts[1] + 2)\n",
    "                else:\n",
    "                    p_y0_mid_x *= (c_xi_and_y0 / self.label_counts[0])\n",
    "                    p_y1_mid_x *= (c_xi_and_y1 / self.label_counts[1])\n",
    "\n",
    "            #calculate argmax\n",
    "            if (p_y0_mid_x > p_y1_mid_x):\n",
    "                preds[row] = 0\n",
    "            else:\n",
    "                preds[row] = 1 \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35b30389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PCA\n",
      "Training accuracy:  0.6113138686131386\n",
      "Testing accuracy:   0.6014492753623188\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(True)\n",
    "nb.fit(x_train, y_train)\n",
    "print(\"My PCA\")\n",
    "print(\"Training accuracy: \", np.mean(nb.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(nb.predict(x_test) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3bfb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK Learn PCA\n",
      "Training accuracy:  0.6113138686131386\n",
      "Testing accuracy:   0.6014492753623188\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(True)\n",
    "nb.fit(x_train, y_train)\n",
    "print(\"SK Learn PCA\")\n",
    "print(\"Training accuracy: \", np.mean(nb.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(nb.predict(x_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b4c58",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a644fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    # Goal: build a tree that recursively paritions feature space such that samples with same labels are grouped together\n",
    "    # Generate candidate split functions G(Q, theta)\n",
    "    # Pick split function that minimises impurity\n",
    "    # use split function to split node\n",
    "    # repeat for splits until max depth is reached: num_features < min_samples or num_features == 1\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        if y.size == 0:\n",
    "            return {'type': 'leaf', 'label': None}\n",
    "        if depth >= self.max_depth or len(set(y)) == 1:\n",
    "            return {'type': 'leaf', 'label': np.bincount(y).argmax()}\n",
    "\n",
    "        feature, threshold = self.choose_feature_threshold(X, y)\n",
    "        left_indices = X[:, feature] <= threshold\n",
    "        right_indices = X[:, feature] > threshold\n",
    "\n",
    "        left_subtree = self.build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self.build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {'type': 'node', 'feature': feature, 'threshold': threshold, 'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "    # Generates candidate split functions and picks the best one\n",
    "    # where best is defined as the one that maximises information gain\n",
    "    def choose_feature_threshold(self, X, y):\n",
    "        best_feature, best_threshold = None, None\n",
    "        best_gain = -1\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                # looking at feature across businesses\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0 or len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self.calculate_gain(y, left_indices, right_indices)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_gain = gain\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # information gain is the difference in entropy before and after split\n",
    "    # G(Q, theta) = H(parent) - (n_left / n_samples) * H(left) - (n_right / n_samples) * H(right)\n",
    "    def calculate_gain(self, y, left_indices, right_indices):\n",
    "        left_weight = len(left_indices) / len(y)\n",
    "        right_weight = len(right_indices) / len(y)\n",
    "        parent_entropy = self.calculate_entropy(y)\n",
    "        left_entropy = self.calculate_entropy(y[left_indices])\n",
    "        right_entropy = self.calculate_entropy(y[right_indices])\n",
    "        gain = parent_entropy - left_weight * left_entropy - right_weight * right_entropy\n",
    "        return gain\n",
    "\n",
    "    # Cross Entropy Loss \n",
    "    def calculate_entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = np.sum(probabilities * -np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node == None:\n",
    "            return 0\n",
    "\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['label']\n",
    "\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self.traverse_tree(x, node['left'])\n",
    "        else:\n",
    "            return self.traverse_tree(x, node['right'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22369a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PCA\n",
      "Training accuracy:  0.6405109489051095\n",
      "Testing accuracy:   0.6014492753623188\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(x_train, y_train)\n",
    "print(\"My PCA\")\n",
    "print(\"Training accuracy: \", np.mean(tree_clf.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(tree_clf.predict(x_test) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3ac637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Depth 0] x7 <= 0\n",
      "  [Depth 1] x1 <= 0\n",
      "    [Depth 2] x0 <= 0\n",
      "      [Depth 3] Leaf: 0\n",
      "      [Depth 3] x9 <= 0\n",
      "        [Depth 4] x6 <= 0\n",
      "          [Depth 5] Leaf: 0\n",
      "          [Depth 5] Leaf: 1\n",
      "        [Depth 4] x3 <= 0\n",
      "          [Depth 5] Leaf: 1\n",
      "          [Depth 5] Leaf: 1\n",
      "    [Depth 2] x3 <= 0\n",
      "      [Depth 3] x4 <= 0\n",
      "        [Depth 4] x9 <= 0\n",
      "          [Depth 5] Leaf: 1\n",
      "          [Depth 5] Leaf: 1\n",
      "        [Depth 4] Leaf: 1\n",
      "      [Depth 3] Leaf: 1\n",
      "  [Depth 1] x5 <= 0\n",
      "    [Depth 2] x3 <= 0\n",
      "      [Depth 3] x2 <= 0\n",
      "        [Depth 4] Leaf: 1\n",
      "        [Depth 4] x8 <= 0\n",
      "          [Depth 5] Leaf: 1\n",
      "          [Depth 5] Leaf: 1\n",
      "      [Depth 3] x9 <= 0\n",
      "        [Depth 4] x0 <= 0\n",
      "          [Depth 5] Leaf: 0\n",
      "          [Depth 5] Leaf: 1\n",
      "        [Depth 4] x2 <= 0\n",
      "          [Depth 5] Leaf: 0\n",
      "          [Depth 5] Leaf: 1\n",
      "    [Depth 2] x2 <= 0\n",
      "      [Depth 3] x0 <= 0\n",
      "        [Depth 4] Leaf: 1\n",
      "        [Depth 4] x3 <= 0\n",
      "          [Depth 5] Leaf: 0\n",
      "          [Depth 5] Leaf: 1\n",
      "      [Depth 3] x4 <= 0\n",
      "        [Depth 4] x8 <= 0\n",
      "          [Depth 5] Leaf: 1\n",
      "          [Depth 5] Leaf: 0\n",
      "        [Depth 4] x3 <= 0\n",
      "          [Depth 5] Leaf: 1\n",
      "          [Depth 5] Leaf: 1\n"
     ]
    }
   ],
   "source": [
    "def print_tree(node, depth=0):\n",
    "    if node['type'] == 'leaf':\n",
    "        print('{}[Depth {}] Leaf: {}'.format('  ' * depth, depth, node['label']))\n",
    "        return\n",
    "\n",
    "    print('{}[Depth {}] x{} <= {}'.format('  ' * depth, depth, node['feature'], node['threshold']))\n",
    "    print_tree(node['left'], depth + 1)\n",
    "    print_tree(node['right'], depth + 1)\n",
    "\n",
    "\n",
    "print_tree(tree_clf.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b9260",
   "metadata": {},
   "source": [
    "### scikit-learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5a91836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.7171532846715328\n",
      "Testing accuracy:   0.572463768115942\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf.fit(x_train, y_train)\n",
    "print(\"Training accuracy: \", np.mean(tree_clf.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(tree_clf.predict(x_test) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98bd3ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.7171532846715328\n",
      "Testing accuracy:   0.572463768115942\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf.fit(x_train, y_train)\n",
    "print(\"Training accuracy: \", np.mean(tree_clf.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(tree_clf.predict(x_test) == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
