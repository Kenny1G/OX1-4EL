{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ff06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import re, string\n",
    "import sys\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82280ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def init_dataset(json) -> tuple[dict, list]:\n",
    "    ds: dict = {}\n",
    "    keys = json.keys()\n",
    "    for k in keys:\n",
    "        ds[k] = []\n",
    "    return ds, keys\n",
    "\n",
    "def read_json(file) -> pd.DataFrame:\n",
    "    dataset = {}\n",
    "    keys = []\n",
    "    with open(file) as file_lines:\n",
    "        for count, line in enumerate(file_lines):\n",
    "            json_line = json.loads(line.strip())\n",
    "            if count == 0:\n",
    "                dataset, keys = init_dataset(json_line)\n",
    "            for k in keys:\n",
    "                dataset[k].append(json_line[k])\n",
    "        return pd.DataFrame(dataset)\n",
    "\n",
    "def read_csv(file) -> pd.DataFrame:\n",
    "    dataset = {}\n",
    "    with open(file, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        keys = reader.fieldnames\n",
    "        for k in keys:\n",
    "            dataset[k] = []\n",
    "        for row in reader:\n",
    "            for k in keys:\n",
    "                dataset[k].append(row[k])\n",
    "    return pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc0257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp_review = read_json('data/yelp_academic_dataset_review.json')\n",
    "yelp_review = read_csv('data/yelp_academic_dataset_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e0a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp_business = read_json('data/yelp_academic_dataset_business.json')\n",
    "yelp_business = read_csv('data/yelp_academic_dataset_business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd32bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data: Restaurants reviewed by karen, the user with the most reviews\n",
    "# Businesses that are categorized as restaurants\n",
    "business_restaurant = yelp_business.loc[yelp_business['categories'].str.contains('Restaurant', na=False)]\n",
    "# Reviews of Restaurant businesses\n",
    "review_restaurant = yelp_review[yelp_review['business_id'].isin(business_restaurant['business_id'])]\n",
    "# User with most restaurant reviews\n",
    "karen = review_restaurant['user_id'].value_counts().index[0]\n",
    "# Reviews Karen has made of restaurant businesses\n",
    "review_restaurant_karen = review_restaurant.loc[review_restaurant['user_id'] == karen]\n",
    "# Restaurant businesses that Karen has reviewed\n",
    "business_restaurant_karen = business_restaurant[business_restaurant['business_id'].isin(review_restaurant_karen['business_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b52787",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Data: remove missing rows and irrelevant columns\n",
    "df = business_restaurant_karen.set_index('business_id')\n",
    "\n",
    "# Remove columns with greater than 20% missing fields\n",
    "mask = df.applymap(lambda x: x =='' or x == 'None').sum()\n",
    "features = ((mask/len(df)) * 100).map(lambda x: x < 20)\n",
    "\n",
    "\n",
    "# Remove non-attribute columns (except business_id)\n",
    "features.loc[~features.index.str.contains('attributes.')] = False\n",
    "#features.loc['business_id'] = True\n",
    "dataset = df.loc[:, features]\n",
    "\n",
    "# Remove rows with missing data\n",
    "mask = dataset.applymap(lambda x: x == '' or x == 'None')\n",
    "dataset = dataset.loc[~mask.any(axis=1)]\n",
    "\n",
    "# Remove all non-boolean columns\n",
    "mask = dataset.applymap(lambda x : x == 'True' or x == 'False').sum() != 0\n",
    "#mask.loc['business_id'] = True\n",
    "#dataset = dataset.set_index('business_id')\n",
    "dataset = dataset.loc[:, mask].applymap(lambda x: x == 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebaeb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Data: add targets\n",
    "df = review_restaurant_karen.set_index('business_id')\n",
    "df = df.loc[df.index.intersection(dataset.index)]\n",
    "df = df.astype({'stars':'float'})\n",
    "dataset['target'] = df.groupby(df.index)['stars'].mean().map(lambda x: x > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "37c04e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lamb = lambda x: 1. if x == True else 0.\n",
    "labels = dataset['target'].map(lamb).to_numpy()\n",
    "feature_set = dataset.drop(['target'], axis=1).applymap(lamb).to_numpy()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(feature_set)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Select the top n principal components\n",
    "n_components = 3\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "tmp_df = pd.DataFrame(X_pca)\n",
    "tmp_df['target'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "83316443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delineate between traning and testing set\n",
    "train_df, test_df = train_test_split(tmp_df, test_size=0.2, random_state=42)\n",
    "y_train = train_df['target'].to_numpy()\n",
    "x_train = train_df.drop(['target'], axis=1).to_numpy()\n",
    "\n",
    "y_test = test_df['target'].to_numpy()\n",
    "x_test = test_df.drop(['target'], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969036f",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4019937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis function... sigmoid function\n",
    "def g(theta, x):\n",
    "    return 1 / (1 + np.exp(-x @ theta))\n",
    "\n",
    "# matrix derivative\n",
    "def dJ(theta, x, y):\n",
    "    m, _ = x.shape\n",
    "    return 1/m* x.T @ (g(theta, x) - y)\n",
    "\n",
    "# hessian matrix\n",
    "def HJ(theta, x):\n",
    "    m, _ = x.shape\n",
    "    Z = g(theta, x)\n",
    "    Z = Z*(1-Z)\n",
    "    return 1/m * Z * x.T @ x\n",
    "\n",
    "# distance between two vectors\n",
    "def dist(x, y):\n",
    "    return np.sum(np.abs(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb794e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, step_size=0.2, max_iter=100, eps=1e-5,\n",
    "                theta_0=None, verbose=True):\n",
    "        self.theta = theta_0\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        m, n = x.shape\n",
    "        if self.theta is None:\n",
    "            self.theta=np.zeros(n)\n",
    "        for i in range(self.max_iter):\n",
    "            theta_new = self.theta - np.linalg.inv(HJ(self.theta, x)) @ dJ(self.theta, x, y)\n",
    "            if dist(theta_new, self.theta) < self.eps:\n",
    "                self.theta = theta_new\n",
    "                break\n",
    "            else:\n",
    "                self.theta = theta_new\n",
    "\n",
    "    def predict(self, x):\n",
    "        return x @ self.theta >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c4ca8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta:  [ 0.06009437 -0.12283594  0.24475308]\n",
      "Training accuracy:  0.5693430656934306\n",
      "Testing accuracy:   0.5797101449275363\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(x_train, y_train)\n",
    "print(\"Theta: \", lg.theta)\n",
    "print(\"Training accuracy: \", np.mean(lg.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(lg.predict(x_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a589d1",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f652b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Naive Bayes Classifier\n",
    "'''\n",
    "class NaiveBayes:\n",
    "    '''\n",
    "    Naive Bayes Classifier (Bernoulli event model)\n",
    "\n",
    "    During training, the classifier learns probabilities by counting the\n",
    "    occurences of feature/label combinations that it finds in the\n",
    "    training data. During prediction, it uses these counts to\n",
    "    compute probabilities.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, use_laplace_add_one):\n",
    "        self.label_counts = {}\n",
    "        self.feature_counts = {}\n",
    "        self.use_laplace_add_one = use_laplace_add_one # True for Laplace add-one smoothing\n",
    "\n",
    "    def fit(self, train_features, train_labels):\n",
    "        '''Training stage - learn from data'''\n",
    "\n",
    "        self.label_counts[0] = 0\n",
    "        self.label_counts[1] = 0\n",
    "\n",
    "        ### YOUR CODE HERE (~5-10 Lines)\n",
    "        self.label_counts[0] = np.count_nonzero(train_labels == 0)\n",
    "        self.label_counts[1] = np.count_nonzero(train_labels == 1)\n",
    "\n",
    "        for row, sample in enumerate(train_features):\n",
    "            label = train_labels[row]\n",
    "            for feature, feature_value in enumerate(sample):\n",
    "                key = (feature, feature_value, label)\n",
    "                self.feature_counts[key] = self.feature_counts.get(key, 0) + 1\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def predict(self, test_features):\n",
    "        '''Testing stage - classify new data'''\n",
    "\n",
    "        preds = np.zeros(test_features.shape[0], dtype=np.uint8)\n",
    "\n",
    "        tot = self.label_counts[0] + self.label_counts[1]\n",
    "        ### YOUR CODE HERE (~10-30 Lines)\n",
    "        p_y0 = self.label_counts[0] / tot\n",
    "        p_y1 = self.label_counts[1] / tot\n",
    "        for row, sample in enumerate(test_features):\n",
    "            p_y0_mid_x = p_y0\n",
    "            p_y1_mid_x = p_y1\n",
    "            for feature, feature_value in enumerate(sample):\n",
    "                #calc prob sample 0\n",
    "                xi = (feature, feature_value, 0)\n",
    "                c_xi_and_y0 = self.feature_counts.get(xi, 0)\n",
    "\n",
    "                #calc prob sample 1 \n",
    "                xi = (feature, feature_value, 1)\n",
    "                c_xi_and_y1 = self.feature_counts.get(xi, 0)\n",
    "\n",
    "                if (self.use_laplace_add_one):\n",
    "                    p_y0_mid_x *= (c_xi_and_y0 + 1) / (self.label_counts[0] + 2)\n",
    "                    p_y1_mid_x *= (c_xi_and_y1 + 1) / (self.label_counts[1] + 2)\n",
    "                else:\n",
    "                    p_y0_mid_x *= (c_xi_and_y0 / self.label_counts[0])\n",
    "                    p_y1_mid_x *= (c_xi_and_y1 / self.label_counts[1])\n",
    "\n",
    "            #calculate argmax\n",
    "            if (p_y0_mid_x > p_y1_mid_x):\n",
    "                preds[row] = 0\n",
    "            else:\n",
    "                preds[row] = 1 \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "35b30389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.7208029197080292\n",
      "Testing accuracy:   0.5362318840579711\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(True)\n",
    "nb.fit(x_train, y_train)\n",
    "print(\"Training accuracy: \", np.mean(nb.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(nb.predict(x_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b9260",
   "metadata": {},
   "source": [
    "### scikit-learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5a91836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.7171532846715328\n",
      "Testing accuracy:   0.5652173913043478\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf = clf.fit(x_train, y_train)\n",
    "print(\"Training accuracy: \", np.mean(tree_clf.predict(x_train) == y_train))\n",
    "print(\"Testing accuracy:  \", np.mean(tree_clf.predict(x_test) == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
